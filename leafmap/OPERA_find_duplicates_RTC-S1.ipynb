{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e98b9b0",
   "metadata": {},
   "source": [
    "# Find duplicate granules in OPERA RTC-S1 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23c9618",
   "metadata": {},
   "outputs": [],
   "source": [
    "import leafmap\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from datetime import datetime\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89393dda",
   "metadata": {},
   "source": [
    "To download and access the data, you will need to create an Earthdata login. You can register for an account at [urs.earthdata.nasa.gov](https://urs.earthdata.nasa.gov)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097cc873",
   "metadata": {},
   "outputs": [],
   "source": [
    "leafmap.nasa_data_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54840ce",
   "metadata": {},
   "source": [
    "## View available OPERA product metadate\n",
    "TSV of NASA Earthdata products is available in the [NASA-Earth-Data](https://github.com/opengeos/NASA-Earth-Data) repo. We filter to just OPERA products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5760a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://github.com/opengeos/NASA-Earth-Data/raw/main/nasa_earth_data.tsv'\n",
    "earth_data_df = pd.read_csv(url, sep='\\t')\n",
    "opera_df = earth_data_df[earth_data_df['ShortName'].str.contains('OPERA', case=False)]\n",
    "opera_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2116d9b8",
   "metadata": {},
   "source": [
    "## Load all RTC-S1 graunules into a geodataframe\n",
    "This will take some time, and quite a bit of RAM. For reference, as of Jan. 2024 there are ~2,100,000 RTC-S1 granules, and it takes somewhere north of 90 minutes to load it into the geodataframe. It also uses significant RAM, so if the kernel crashes, it may be best to reduce the number of files you are reading into memory. Change `count` to something other than -1 (all files)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186bf525",
   "metadata": {},
   "outputs": [],
   "source": [
    "results, gdf = leafmap.nasa_data_search(\n",
    "    short_name='OPERA_L2_RTC-S1_V1',\n",
    "    cloud_hosted=True,\n",
    "    bounding_box= (-180.0, -90.0, 180, 90.0), # \n",
    "    temporal=(\"2014-06-15\", str(datetime.now().date())),\n",
    "    count=-1,  # use -1 to return all datasets\n",
    "    return_gdf=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031dfdcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8f5256",
   "metadata": {},
   "source": [
    "### Make a list of 'native-id' from the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c4a73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "identifier_list = gdf['native-id'].tolist()\n",
    "print('Total granules:', len(identifier_list))\n",
    "print(identifier_list[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d43540",
   "metadata": {},
   "source": [
    "## Find duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c737c6",
   "metadata": {},
   "source": [
    "### Isolate the part of the file name that would be the same for duplicate granules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804eafa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Access the parts that may indicate true duplicates from the identifier name\n",
    "print(identifier_list[0][0:-29])   # burst ID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc24462",
   "metadata": {},
   "source": [
    "### Create a set of duplicates and unique identifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ac6e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "### This is only the hdf5 \n",
    "duplicate_identifiers = set()\n",
    "unique_identifiers = set()\n",
    "\n",
    "for identifier in identifier_list:\n",
    "    potential_duplicate_portion = identifier[0:-29]\n",
    "\n",
    "    # Check if the identifier is already in the set\n",
    "    if potential_duplicate_portion in unique_identifiers:\n",
    "        duplicate_identifiers.add(potential_duplicate_portion)\n",
    "    else:\n",
    "        # Add the identifier to the set if it's not a duplicate\n",
    "        unique_identifiers.add(potential_duplicate_portion)\n",
    "\n",
    "# If you need the result as a list, you can convert the sets back to lists\n",
    "duplicate_identifiers_list = list(duplicate_identifiers)\n",
    "unique_identifiers_list = list(unique_identifiers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c33b1e",
   "metadata": {},
   "source": [
    "### Print the basic stats as of today (total duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3b50d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Total RTC-S1 granules as of {datetime.now().strftime(\"%d-%m-%Y\")}:', len(identifier_list))\n",
    "print('Granules with more than one version:',len(duplicate_identifiers))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6962c147",
   "metadata": {},
   "source": [
    "### Collect duplicates for each granulate where duplicates exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faae27c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to store the potentially duplicated portion as the key and a list of entire elements as the value\n",
    "granules_dictionary = {}\n",
    "\n",
    "# Create a list to store pairs of potentially duplicated elements\n",
    "duplicate_pairs = []\n",
    "\n",
    "# Iterate over the elements in the list\n",
    "for granule in identifier_list:\n",
    "    # Extract the potentially duplicated portion\n",
    "    potential_duplicate_portion = granule[0:-29]\n",
    "    \n",
    "    # If the potential duplicate portion is not in the dictionary, add it with the entire element\n",
    "    if potential_duplicate_portion not in granules_dictionary:\n",
    "        granules_dictionary[potential_duplicate_portion] = [granule]\n",
    "    else:\n",
    "        # If the potential duplicate portion is already in the dictionary, add the entire element to the list\n",
    "        granules_dictionary[potential_duplicate_portion].append(granule)\n",
    "\n",
    "# Create pairs from the dictionary values\n",
    "for granules in granules_dictionary.values():\n",
    "    if len(granules) > 1:\n",
    "        duplicate_pairs.append(granules)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4e29da",
   "metadata": {},
   "source": [
    "### Print the number of duplicates for each granule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebfaf12",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Total RTC-S1 granules as of {datetime.now().strftime(\"%d-%m-%Y\")}:', len(identifier_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fa2c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = []\n",
    "for pair in duplicate_pairs:\n",
    "    for granule in pair:\n",
    "        duplicates.append(granule)\n",
    "print(f'Total RTC-S1 duplicate tiles as of {datetime.now().strftime(\"%d-%m-%Y\")}: {len(duplicates)} granules')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1f4ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_duplicate = []\n",
    "for pair in duplicate_pairs:\n",
    "    if len(pair) == 2:\n",
    "        one_duplicate.append(pair)\n",
    "print(f'Total RTC-S1 duplicate tiles with 1 duplicate as of {datetime.now().strftime(\"%d-%m-%Y\")}: {len(one_duplicate)} totaling {len(one_duplicate)*2} granules.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3fc8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "two_duplicates = []\n",
    "for pair in duplicate_pairs:\n",
    "    if len(pair) == 3:\n",
    "        two_duplicates.append(pair)\n",
    "print(f'Total RTC-S1 duplicate tiles with 2 duplicate as of {datetime.now().strftime(\"%d-%m-%Y\")}: {len(two_duplicates)} totaling {len(two_duplicates)*3} granules.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9fbb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "three_duplicates = []\n",
    "for pair in duplicate_pairs:\n",
    "    if len(pair) == 4:\n",
    "        three_duplicates.append(pair)\n",
    "print(f'Total RTC-S1 duplicate tiles with 3 duplicate as of {datetime.now().strftime(\"%d-%m-%Y\")}: {len(three_duplicates)} totaling {len(three_duplicates)*4} granules.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0cddae",
   "metadata": {},
   "outputs": [],
   "source": [
    "four_duplicates = []\n",
    "for pair in duplicate_pairs:\n",
    "    if len(pair) == 5:\n",
    "        four_duplicates.append(pair)\n",
    "print(f'Total RTC-S1 duplicate tiles with 4 duplicate as of {datetime.now().strftime(\"%d-%m-%Y\")}: {len(four_duplicates)} totaling {len(four_duplicates)*5} granules.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f531b82f",
   "metadata": {},
   "source": [
    "### Add the url, burst IDs, and dates to the duplicate names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ba1e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_urls = []\n",
    "for pair in duplicate_pairs:\n",
    "    pair_urls = []\n",
    "    for granule in pair:\n",
    "        pair_urls.append('https://datapool.asf.alaska.edu/RTC/OPERA-S1/'+str(granule)+'.h5')\n",
    "    duplicate_urls.append(pair_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fab8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "burst_ids = []\n",
    "dates = []\n",
    "for pair in duplicate_pairs:\n",
    "    burst_ids.append(pair[0][16:31])\n",
    "    date_str = pair[0][32:48]\n",
    "    datetime_object = datetime.strptime(date_str, '%Y%m%dT%H%M%SZ')  \n",
    "    dates.append(datetime_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde71dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates_df = pd.DataFrame({\n",
    "    'burst_id': burst_ids,\n",
    "    'acquisition_date': dates,\n",
    "    'duplicates': duplicate_urls\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c1c659",
   "metadata": {},
   "source": [
    "### Make dataframe of all duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3eb392a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_final = pd.DataFrame(duplicates_df['duplicates'].tolist(), index=duplicates_df[['burst_id', 'date']]).reset_index()\n",
    "df_final = pd.concat([duplicates_df[['burst_id', 'acquisition_date']], duplicates_df['duplicates'].apply(lambda x: pd.Series(x))], axis=1)\n",
    "\n",
    "# Rename the columns\n",
    "df_final.columns = ['burst_id', 'acquisition_date', 'duplicate_1', 'duplicate_2']\n",
    "#df_final.columns = ['burst_id', 'acquisition_date', 'duplicate_1', 'duplicate_2', 'duplicate_3','duplicate_4']\n",
    "\n",
    "# Sort by burst_id \n",
    "sorted_df = df_final.sort_values(by='burst_id', ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f525e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104c54a2",
   "metadata": {},
   "source": [
    "### Add columns of interest from the original geodataframe for each duplicate and format it nicely (probably could be improved, but should work)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5dd9a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(gdf)\n",
    "df2.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b341c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_portion(url):\n",
    "    if pd.isna(url):\n",
    "        return None\n",
    "    match = re.search(r'/([^/]+)\\.h5', url)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "# Apply the function to extract the portion and create a new column\n",
    "# The try/except approach is used in case there less than 4 duplicates found for every granule\n",
    "\n",
    "try:\n",
    "    sorted_df['extracted_portion_duplicate_1'] = sorted_df['duplicate_1'].apply(extract_portion)\n",
    "    sorted_df['extracted_portion_duplicate_2'] = sorted_df['duplicate_2'].apply(extract_portion)\n",
    "    sorted_df['extracted_portion_duplicate_3'] = sorted_df['duplicate_3'].apply(extract_portion)\n",
    "    sorted_df['extracted_portion_duplicate_4'] = sorted_df['duplicate_4'].apply(extract_portion)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    merged_df1 = pd.merge(sorted_df, df2, left_on='extracted_portion_duplicate_1', right_on='native-id', how='inner')\n",
    "    merged_df2 = pd.merge(sorted_df, df2, left_on='extracted_portion_duplicate_2', right_on='native-id', how='inner')\n",
    "    merged_df3 = pd.merge(sorted_df, df2, left_on='extracted_portion_duplicate_3', right_on='native-id', how='inner')\n",
    "    merged_df4 = pd.merge(sorted_df, df2, left_on='extracted_portion_duplicate_4', right_on='native-id', how='inner')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "columns_to_drop = ['acquisition_date','size','concept-type','concept-id','Projects','ArchiveAndDistributionInformation','DayNightFlag',\n",
    "                   'Identifiers','Platforms','Name','URL','ShortName','AdditionalAttributes','GPolygons','ProviderDates','EntryTitle','PGEName','PGEVersion',\n",
    "                   'OrbitCalculatedSpatialDomains','GranuleUR','RelatedUrls','InputGranules','format','provider-id','native-id','revision-id']\n",
    "\n",
    "try:\n",
    "    merged_df1 = merged_df1.drop(columns=columns_to_drop)\n",
    "    merged_df2 = merged_df2.drop(columns=columns_to_drop)\n",
    "    merged_df3 = merged_df3.drop(columns=columns_to_drop)\n",
    "    merged_df4 = merged_df4.drop(columns=columns_to_drop)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    merged_df1['revision-date-1'] = merged_df1['revision-date']\n",
    "    merged_df2['revision-date-2'] = merged_df2['revision-date']\n",
    "    merged_df3['revision-date-3'] = merged_df3['revision-date']\n",
    "    merged_df4['revision-date-4'] = merged_df4['revision-date']\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    merged_df1['ProductionDateTime-1'] = merged_df1['ProductionDateTime']\n",
    "    merged_df2['ProductionDateTime-2'] = merged_df2['ProductionDateTime']\n",
    "    merged_df3['ProductionDateTime-3'] = merged_df3['ProductionDateTime']\n",
    "    merged_df4['ProductionDateTime-4'] = merged_df4['ProductionDateTime']\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    merged_df = pd.merge(merged_df1, merged_df2, on='duplicate_1', how='left', suffixes=('_df2', '_df1'))\n",
    "    merged_df = pd.merge(merged_df, merged_df3, on='duplicate_1', how='left', suffixes=('_merged', '_df3'))\n",
    "    merged_df = pd.merge(merged_df, merged_df4, on='duplicate_1', how='left', suffixes=('_merged', '_df4'))\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    merged_df['burst_id'] = merged_df['burst_id_df2']\n",
    "    merged_df['duplicate_2'] = merged_df['duplicate_2_df2']\n",
    "    merged_df['duplicate_3'] = merged_df['duplicate_3_df2']\n",
    "    merged_df['duplicate_4'] = merged_df['duplicate_4_df2']\n",
    "except:\n",
    "    pass\n",
    "\n",
    "suffixes_to_remove = ['_df1', '_df2', '_df3', '_df4','merged']\n",
    "\n",
    "# Iterate over the suffixes and drop columns\n",
    "for suffix in suffixes_to_remove:\n",
    "    try:\n",
    "        columns_to_drop = [col for col in merged_df.columns if (col.endswith(suffix))]\n",
    "        merged_df = merged_df.drop(columns=columns_to_drop)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "merged_df['extracted_portion_duplicate_1'] = merged_df['duplicate_1'].apply(extract_portion)\n",
    "\n",
    "common_column = 'extracted_portion_duplicate_1'\n",
    "column_to_include = 'BeginningDateTime'\n",
    "\n",
    "# Merge the DataFrames based on the common column\n",
    "merged_df = pd.merge(merged_df, df2[['native-id', column_to_include]], left_on=common_column, right_on='native-id', how='left')\n",
    "\n",
    "# Drop the duplicate columns and rename the result column\n",
    "merged_df = merged_df.drop(columns=['native-id','extracted_portion_duplicate_1']).rename(columns={column_to_include: 'BeginningDateTime'})\n",
    "\n",
    "# Specify the desired column order\n",
    "desired_order = ['burst_id', 'duplicate_1', 'duplicate_2', 'duplicate_3','duplicate_4','BeginningDateTime','revision-date-1','revision-date-2','revision-date-3','revision-date-4','ProductionDateTime-1','ProductionDateTime-2','ProductionDateTime-3','ProductionDateTime-4']\n",
    "\n",
    "# Filter the columns that exist in the DataFrame\n",
    "existing_columns = [col for col in desired_order if col in merged_df.columns]\n",
    "\n",
    "# Reorder the DataFrame columns based on the desired order\n",
    "merged_df = merged_df[existing_columns]\n",
    "\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35783a7",
   "metadata": {},
   "source": [
    "## Output dataframe as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086080be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the DataFrame to a CSV file\n",
    "csv_file_path = f'RTC-S1_duplicates_{datetime.now().strftime(\"%d-%m-%Y\")}.csv'\n",
    "merged_df.to_csv(csv_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7df56f",
   "metadata": {},
   "source": [
    "## Output the GeoDataFrame to GeoJSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08696bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make geodataframe\n",
    "duplicates_gdf = gdf[gdf['native-id'].isin(duplicates)]\n",
    "\n",
    "columns_to_drop = ['size', 'concept-type', 'concept-id', 'revision-id', 'native-id',\n",
    "       'provider-id', 'format', 'revision-date', 'BeginningDateTime',\n",
    "       'EndingDateTime', 'OrbitCalculatedSpatialDomains', 'GranuleUR',\n",
    "       'AdditionalAttributes', 'GPolygons', 'ProviderDates', 'EntryTitle',\n",
    "       'PGEName', 'PGEVersion', 'RelatedUrls', 'InputGranules', 'Projects',\n",
    "       'ArchiveAndDistributionInformation', 'DayNightFlag', 'Identifiers',\n",
    "       'ProductionDateTime', 'Platforms', 'Name', 'URL', 'ShortName',]\n",
    "duplicates_gdf = duplicates_gdf.drop(columns = columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7622d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates_gdf.to_file(f'RTC-S1_duplicates_{datetime.now().strftime(\"%d-%m-%Y\")}.geojson', driver='GeoJSON')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
